---
title: "Best Practice"
slug: "/best_practice"
---

import useBaseUrl from '@docusaurus/useBaseUrl';

# Best Practice

## Introduction

All scientists generate scientific data independently of whether they are working practically or theoretically, and produce data such as reaction protocols, analytical data, or data from calculations. These data can be [domain-specific](/docs/domain_guide) and should be always accompanied by [metadata](/docs/metadata) to describe that data.

In fact, all scientists are responsible for [organisation](/docs/data_organisation) and [documentation](/docs/data_documentation) of their data in order to ensure that the data can be archived in a [FAIR](/docs/fair) way. For good data handling practices, the use of [data management plans](/docs/dmp) is recommended. Most research and funding institutions provide internal guidelines for research data management (RDM), such as the [DFG](https://www.dfg.de/download/pdf/foerderung/grundlagen_dfg_foerderung/forschungsdaten/forschungsdaten_checkliste_de.pdf). Many funding institutions encourage or even enforce the [storage](/docs/data_storage) and [publication](/docs/data_publication) of research data in repositories.

## Usage of digital environments

Over the last years, many new digital tools have been developed to support researchers in their RDM needs. The use of [electronic lab notebooks (ELNs)](/docs/eln) is essential to support scientists in their daily work to collected data in a structured way. By using an ELN for experiment planning and documentation, it is guaranteed that the data is directly digitally available. Best practice includes the use of an ELN that covers the entire [data life cycle](/docs/data_life_cycle) – starting with experiment planning, passing through data collection, processing, and analysis to publication and reuse.

The daily work can all be performed in one digital environment. Nevertheless, also other software of choice can be used to process analytical data which can then be added to the corresponding ELN entry. If the ELN used is directly connected to a [repository](/docs/repositories), the structured data obtained can be directly transferred to a repository. This greatly eases [data publication](/docs/data_publication). This could also make the publication of supplementary materials PDFs obsolete in the near future.

Generally, selecting an ELN and a repository – generic or field-specific – depends on the [chemical sub-domain](/docs/domain_guide). The combination of both enables effective [data storage and archiving](/docs/data_storage) in terms of internal databases and data publications for advanced collaboration and data reuse, and fulfils the data handling practices of the [data life cycle](/docs/data_life_cycle). Moreover, the storage of research data according to the FAIR principles is crucial for upcoming machine learning approaches or big data analyses.

:::danger Notice:
Field-specific repositories should be the first choice as these repositories enhance the FAIRness of data on behalf of the submitters. To retain the same level of FAIRness, data publication in generic repositories requires manual FAIRification.
:::

## How to use dataset DOIs in publications

During deposition of research data, a [persistent identifier](/docs/pid) is assigned to the data. There are two main options to use that PIDs:

- add the PID to the [Data Availability Statement](/docs/data_availability_statement) of the article, if the dataset is the corresponding, hence, related dataset of a scientific article.
- add the PID to the reference section of the manuscript, if the dataset is a dataset of other researchers, which is reuse and cited as reference.

:::info Notice:
This distinction is important, because the link to the dataset in the DOI metadata of the article is differently set, depending on whether the dataset is a directly related source of information or a referenced source of other researchers and its information is reused.
:::

Additionally, the PID of a related dataset could be additionally mentioned in the supplementary PDF, if such a document is generated.

## Best practice for field-specific repositories

Using a field-specific repository such as [Chemotion ELN](https://www.chemotion.net/docs) in combination with its repository [Chemotion Repository](https://www.chemotion-repository.net/welcome) realises efficient data handling for synthetically working chemists. The entire Chemotion package allows to collect, analyse, process, store, and publish various types of analytical data attached to reaction and samples in one digital environment.

During the seamless export from the ELN into the repository, [persistent identifiers (DOIs)](/docs/pid) are assigned to the deposited data. Chemotion Repository [provides](https://www.chemotion.net/docs/repo/doi)) DOIs for reactions, samples and analysis data. Moreover, Collection DOIs can be assigned for a set of reactions, samples and analyses to assemble a study, which may also has its own scientific article with one of the scientific publishers.

Additionally,  Chemotion Repository is connected to other databases (e.g. [PubChem](https://pubchem.ncbi.nlm.nih.gov/)) and repositories (e.g. [NMRShiftDB2](https://nmrshiftdb.nmr.uni-koeln.de/), superseded soon by [nmrXiv](https://nmrxiv.org/)) to ensure best visibility and a user-friendly search of original research data.

:::danger Notice:
Are you asking yourself right now, "That's it?" Yes, that's it! As mentioned above, FAIRification is mainly done by the field-specific repository and structured data is collected along the scientific workflow.
:::

<!--- ![BestPractice](/img/topics/BestPractice.png) <br/>
**Figure 1:** Section of the experimental part in journal publication DOI: [10.1039/d1dt00832c](https://doi.org/10.1039/D1DT00832C), citation of data publication marked in orange. Ti: This should not be the way to go which should be highlighted with a figure--->

 Several working groups of NFDI4Chem and beyond already deposit research data in the [Chemotion-Repository](https://www.chemotion-repository.net/welcome). For a better understanding of how data publications are linked to related journal publications, the following examples including their data (open-access) can be viewed:

- [Modular Synthesis of trans‐A2B2‐Porphyrins with Terminal Esters: Systematically Extending the Scope of Linear Linkers for Porphyrin‐Based MOFs](https://doi.org/10.1002/chem.202003885)
- [Next Generation of Zinc Bisguanidine Polymerization Catalysts towards Highly Crystalline, Biodegradable Polyesters](https://doi.org/10.1002/anie.202008473)
- [Synthesis of new pyrazolo[1,2,3]triazines by cyclative cleavage of pyrazolyltriazenes](https://doi.org/10.3762/bjoc.17.187)
- [Exceptional Substrate Diversity in Oxygenation Reactions Catalyzed by a Bis(μ-oxo) Copper Complex](http://dx.doi.org/10.1002/chem.202000664)
- [Insertion of [1.1.1]propellane into aromatic disulfides](https://doi.org/10.3762/bjoc.15.114) Ti: We might move this to Chemotion Documentation

## Best practice for generic repositories

For field-specific repositories such as Chemotion ELN/Repository, the data is collected along the scientific workflows, analytical data files are automatically converted to open formats, controlled vocabularies or ontologies are used to describe data, and analysis data is interconnected to reactions and samples, hence, chemical structures. By deciding to publish research data in generic repositories, data is usually gathered contemporaneous with the preparation of a corresponding manuscript, as long as no institutional RDM workflows streamline data management. Hence, datasets which are planned to be published with a generic repository require manual FAIRification prior upload and publishing.

To start, all data to be published is collected and ordered in a logical folder structure, e.g. a folder for NMR and another folder for MS data (figure 1). Nested folder structures should be avoided and aspects of [data organisation](/docs/data_organisation), such as file naming, should be taken into account. Researchers should aim for a data package which is self-explaining as the supplementary PDFs they have previously published along scientific articles.

<img align="center" src={useBaseUrl('/img/data_pub/best_practice_generic_folders_content.png')} alt="" width="80%" />

**Figure 1:** Folder structure and content of the [Lead-by-Example](/docs/lbe_intro) dataset of [Linderazuelene](/docs/datasets/?doi=10.1002/ciuz.201900868).

For further aspects to consider, we provide a non-exhaustive list on what the chemistry dataset should contain and how the chemical data should be represented:

- Open formats should be used and original raw data in proprietary formats should be converted to open formats:<br/>[Open formats](/docs/format_standards) for analytical data should be the main choice. However, many analytical instruments provide data in proprietary formats. Not all data in these formats is necessarily also included in the selected open format it was converted to, depending on the specification of the open format. Hence, the original raw data in proprietary formats should also be published, although this data may have limited interoperability. Publishing original raw data is a measure of scientific integrity and allows for unbiased reprocessing and reuse of that data. If no open format exists, export as text file, i.e. without any format specification, should be considered.
- Linking analytical data files to chemical structures:<br/>As analytical data are usually named following lab journal entries, the dataset needs to contain a description on which data corresponds to which sample, molecule or experiment. This should be included in the format used for analytical data. Another solution on that is to provide a [supplementary table](/docs/machine-readable_chemical_structures/#provide-machine-readable-data-as-supplementary-table) (figure 1) within the data package, which should include InChI structure identifiers and SMILES structure codes and additional information such as RInChI reaction identifiers. Optionally, chemical structures might also be added and represented as [CT files](/docs//ct_files) such as mol or SDfiles (figure 1). [RXNfiles](/docs//ct_files) may be used to describe chemical reactions in a machine readable way.
- Scripts and Workflows:<br/>Workflows, i.e. scripts used for data processing and input parameters, also for semi-automatic scripts, should be included. The metadata of the whole dataset should describe the language and version of scripts used as well as other software applied. Best practice is not only to include the code but to add notebooks such as R Notebooks or Jupyther notebooks.
- Provenance information should be included:<br/>Part of that [provenance information](/docs/provenance) is part of the dataset's metadata and should be added via the metadata editor of the repository. All information which were previously included in the supplementary PDFs section on general information, e.g. information on methods and instruments used as well as reaction protocols, are provenance information and should be added to a README file. This README could be a text file, or written in Markdown while also providing a human-readable rendered representation as HTML.

:::danger Notice:
Does that sound like a lot of manual work? Avoid extra work by using digital environments for collecting, processing, analysing and publishing research data! Plus, you may omit the preparation of supplementary PDFs and use the saved work time to get your dataset ready for publication!
:::

----
Main authors: [ORCID:0000-0003-4480-8661](https://orcid.org/0000-0003-4480-8661) and [ORCID:0000-0003-2060-842X](https://orcid.org/0000-0003-2060-842X)
